{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a8d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('README.md', 'r') as f:\n",
    "    count=0\n",
    "    for line in f:\n",
    "        if 'spark' in line.lower():\n",
    "            count +=1\n",
    "\n",
    "        \n",
    "print(f\"Masz {count} lini zawierających spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/air/Desktop/spark/')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"spark1\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "lines = sc.textFile('README.md')\n",
    "count_s = lines.filter(lambda x: 'spark' in x.lower())\n",
    "print(f\"Masz {count_s.count()} lini zawierających spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6877848",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file spark1.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    spark = SparkSession.builder\\\n",
    "        .appName(\"spark1\")\\\n",
    "        .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    lines = sc.textFile('README.md')\n",
    "    spark_count = lines.filter(lambda x: 'spark' in x.lower())\n",
    "    print(f\"Masz {spark_count.count()} wierszy z wyrazem spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40aba1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! spark-submit spark1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d9be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "person1 = Row(age=34, name='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7847666",
   "metadata": {},
   "outputs": [],
   "source": [
    "person1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccfa652",
   "metadata": {},
   "outputs": [],
   "source": [
    "person1['name'], person1.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'name' in person1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26695a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "newPerson = Row(\"age\",\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b6650",
   "metadata": {},
   "outputs": [],
   "source": [
    "person2 = newPerson(24,\"Alicja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "person3 = newPerson(None, None)\n",
    "person4 = newPerson(33, None)\n",
    "person5 = newPerson(None, 'Peter')\n",
    "person6 = newPerson(32, 'Peter')\n",
    "person7 = newPerson(40, 'Greg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846586be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = [person1, person2, person3, person4, \n",
    "                                  person5, person6, person7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85643084",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF = spark.createDataFrame(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274dadce",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cfc728",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdbb81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56345f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"V1\", IntegerType()), StructField(\"V2\", StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d9874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1,2),(3,4),(5,6)], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafee0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92829da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c4499",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ccb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file example.json\n",
    "\n",
    "'''\n",
    "{\"string\":\"string1\",\"int\":1,\"array\":[1,2,3],\"dict\": {\"key\": \"value1\"}}\n",
    "{\"string\":\"string2\",\"int\":2,\"array\":[2,4,6],\"dict\": {\"key\": \"value2\"}}\n",
    "{\"string\":\"string3\",\"int\":3,\"array\":[3,6,9],\"dict\": {\"key\": \"value3\", \"extra_key\": \"extra_value3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da76c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = spark.read.json('example.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cef611",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e94e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"RDD_input\")\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee1ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['line']\n",
    "df = df.toDF(*col_names)\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ab31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df.select(f.explode(f.split(df.line, \" \")).alias(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fafdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb345690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/air/Desktop/spark/')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b28606",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\") \\\n",
    "        .appName(\"Stream_DF\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c2104",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.readStream\\\n",
    "        .format(\"socket\")\\\n",
    "        .option(\"host\", \"localhost\")\\\n",
    "        .option(\"port\", 9999)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730963fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.select(f.explode(f.split(lines.value, \" \")).alias(\"word\"))\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "query = wordCounts.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "query.awaitTermination()\n",
    "query.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683214cd",
   "metadata": {},
   "source": [
    "lines = DataFrame reprezentujący nieograniczoną tabelę zawierającą dane strumieniowe. \n",
    "Zawiera ona jedną kolumnę o nazwie `value`. Każda nowa linia to wiersz w tabeli. \n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9536c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! spark-submit spark2.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
