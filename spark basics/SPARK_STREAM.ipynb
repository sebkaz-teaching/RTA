{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a6ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/air/Desktop/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11339aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e247d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .appName(\"new\")\\\n",
    "        .getOrCreate()\n",
    "# otrzymanie obiektu SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabfe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Word Count on RDD \n",
    "sc.textFile(\"RDD_input\") \\\n",
    ".map(lambda x: re.findall(r\"[a-z']+\", x.lower())) \\\n",
    ".flatMap(lambda x: [(y, 1) for y in x]) \\\n",
    ".reduceByKey(lambda x,y: x + y) \\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004acd25",
   "metadata": {},
   "source": [
    "## SPARK STREAMING\n",
    "\n",
    "Część Sparka odpowiedzialna za przetwarzanie danych w czasie rzeczywistym. \n",
    "\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-arch.png\"/>\n",
    "\n",
    "Dane mogą pochodzić z różnych źródeł np. sokety TCP, Kafka, etc. \n",
    "Korzystając z poznanych już metod `map, reduce, join, oraz window` można w łatwy sposób generować przetwarzanie strumienia tak jaby był to nieskończony ciąg RDD. \n",
    "Ponadto nie ma problemu aby wywołać na strumieniu operacje ML czy wykresy. \n",
    "\n",
    "Cała procedura przedstawia się następująco: \n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-flow.png\"/>\n",
    "\n",
    "SPARK STREAMING w tej wersji wprowadza abstrakcje zwaną `discretized stream` *DStream* (reprezentuje sekwencję RDD).\n",
    "\n",
    "Operacje na DStream można wykonywać w API JAVA, SCALA, Python, R (nie wszystkie możliwości są dostępne dla Pythona). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b1ebc",
   "metadata": {},
   "source": [
    "Spark Streaming potrzebuje minium 2 rdzenie.\n",
    "\n",
    "----\n",
    "- **StreamingContext(sparkContext, batchDuration)** - reprezentuje połączenie z klastrem i służy do tworzenia DStreamów, `batchDuration` wskazuje na granularność batch'y (w sekundach)\n",
    "- **socketTextStream(hostname, port)** - tworzy DStream na podstawie danych napływających ze wskazanego źródła TCP\n",
    "- **flatMap(f), map(f), reduceByKey(f)** - działają analogicznie jak w przypadku RDD z tym że tworzą nowe DStream'y\n",
    "- **pprint(n)** - printuje pierwsze `n` (domyślnie 10) elementów z każdego RDD wygenerowanego w DStream'ie\n",
    "- **StreamingContext.start()** - rozpoczyna działania na strumieniach\n",
    "- **StreamingContext.awaitTermination(timeout)** - oczekuje na zakończenie działań na strumieniach\n",
    "- **StreamingContext.stop(stopSparkContext, stopGraceFully)** - kończy działania na strumieniach\n",
    "\n",
    "Obiekt StreamingContext można wygenerować za pomocą obiektu SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6356fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/air/Desktop/spark')\n",
    "\n",
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread\n",
    "# and batch interval of 1 second\n",
    "\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "\n",
    "ssc = StreamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b9c6c",
   "metadata": {},
   "source": [
    "Po wygenerowaniu obiektu `ssc` musisz wskazaź źródło i utworzyć na jego podstawie DStream. Określić wszystkie transformacje. Uruchomić metodę `start()`, która powoduje nasłuchiwanie. Włączyć oczekiwanie na zakończenie procesu `awaitTermination()` bądź zatrzymać nasłuch ręcznie `stop()`. \n",
    "\n",
    "- po rozpoczęciu nasłuchu nie można już ustawić nowych przekształceń !\n",
    "- po zatrzymaniu nie można zrestartować\n",
    "- tylko jeden StreamingContext aktywny na JVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DStream\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "# podziel każdą linię na wyrazy\n",
    "# DStream jest mapowany na kolejny DStream\n",
    "# words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "words = lines.flatMap(lambda x: re.findall(r\"[a-z']+\", x.lower()))\n",
    "\n",
    "# zliczmy każdy wyraz w każdym batchu\n",
    "# DStream jest mapowany na kolejny DStream\n",
    "# pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "# DStream jest mapowany na kolejny DStream                  \n",
    "# wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "wordCounts = words.map(lambda word: (word,1)).reduceByKey(lambda x,y: x+y)\n",
    "# wydrukuj pierwszy elemnet\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate\n",
    "ssc.stop(True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w konsoli linuxowej netcat Nmap for windows\n",
    "!nc -lk 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35cc81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/air/Desktop/spark')\n",
    "\n",
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread\n",
    "# and batch interval of 1 second\n",
    "\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount2\")\n",
    "ssc = StreamingContext(sc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef650ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file stream.py\n",
    "\n",
    "from socket import *\n",
    "import time\n",
    "\n",
    "rdd = list()\n",
    "with open(\"RDD_input\", 'r') as ad:\n",
    "    for line in ad:\n",
    "        rdd.append(line)\n",
    "\n",
    "HOST = 'localhost'\n",
    "PORT = 9999\n",
    "ADDR = (HOST, PORT)\n",
    "tcpSock = socket(AF_INET, SOCK_STREAM)\n",
    "tcpSock.bind(ADDR)\n",
    "tcpSock.listen(5)\n",
    "\n",
    "\n",
    "while True:\n",
    "    c, addr = tcpSock.accept()\n",
    "    print('got connection')\n",
    "    for line in rdd:\n",
    "        try:\n",
    "            c.send(line.encode())\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            break\n",
    "    c.close()\n",
    "    print('disconnected')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uruchom w konsoli \n",
    "!python stream.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd41af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "words = lines.flatMap(lambda x: re.findall(r\"[a-z']+\", x.lower()))\n",
    "wordCounts = words.map(lambda word: (word,1)).reduceByKey(lambda x,y: x+y)\n",
    "# wydrukuj pierwszy elemnet\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate\n",
    "ssc.stop(True,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cffa64",
   "metadata": {},
   "source": [
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream.png\"/>\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26701ee4",
   "metadata": {},
   "source": [
    "W celach ćwiczeniowych i testowania przetwarzania strumieniowego warto wykorzystać Obiekt Kolejki wygenerowany z obiektów RDD.\n",
    "Każdy RDD wepchany (pushed) do kolejki traktowany jest jako batch w DStream i przetwarzany jest jako strumień. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba5c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/air/Desktop/spark')\n",
    "\n",
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread\n",
    "# and batch interval of 1 second\n",
    "\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount3\")\n",
    "ssc = StreamingContext(sc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df6514e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "[j for j in range(1, 1001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3267d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddQueue = []\n",
    "for i in range(10):\n",
    "        rddQueue += [sc.parallelize(\n",
    "            [j for j in range(1, 1001)], 10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0911eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2426dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputStream = ssc.queueStream(rddQueue)\n",
    "\n",
    "mappedStream = inputStream.map(lambda x: (x % 10, 1))\n",
    "reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "reducedStream.pprint()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737203f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run  \n",
    "import time\n",
    "ssc.start()\n",
    "time.sleep(10)\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c7459",
   "metadata": {},
   "source": [
    "## Stateful Wordcount \n",
    "\n",
    "Operacja `updateStateByKey` pozwala łączyć ze sobą wyniki otrzymywane na poszczególbych DStreamach. Dzięki tej operacji możesz w sposób ciągły uzupełniać informacje !\n",
    "\n",
    "Aby Spark Streaming mógł łączyć dane z wielu batchy (stateful transformations) konieczne jest wskazanie lokalizacji gdzie zapisywane będą checkpointy.\n",
    "\n",
    "1. Zdefiniuj stan podstawowy\n",
    "2. wskaż funkcję łączącą \n",
    "\n",
    "----\n",
    "- **checkpoint(directory)** - wskazuje gdzie zapisywane będą checkpointy z operacji na DStream'ach\n",
    "- **updateStateByKey(updateFunc)** - zwraca nowy DStream zawierający informację o bieżącym stanie poszczególnych kluczy, stan każdego klucza odświeżany jest przy pomocy `updateFunc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9027dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba710cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/air/Desktop/spark')\n",
    "\n",
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread\n",
    "# and batch interval of 1 second\n",
    "\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWC3\")\n",
    "ssc = StreamingContext(sc, 5)\n",
    "ssc.checkpoint(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "words = lines.flatMap(lambda x: re.findall(r\"[a-z']+\", x.lower()))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "runningCounts = pairs.updateStateByKey(updateFunc)\n",
    "\n",
    "runningCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64639d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "\n",
    "ssc.stop(True,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae57f4",
   "metadata": {},
   "source": [
    "> Zadanie - Korzystając z danych kolejki rddQueue dodaj wszystkie elementy do siebie "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab1ce22",
   "metadata": {},
   "source": [
    "## Redukcja w oknach \n",
    "\n",
    "----\n",
    "- **reduceByKeyAndWindow(func, invFunc, windowDuration, slideDuration)** - zwraca nowy DStream powstały w wyniku stosowania przyrostowo reduceByKey wewnątrz zdefiniowanego okna. Zredukowane wartości dla nowego okna obliczane są z wykorzystaniem wartości starego okna poprzez: \n",
    "1. zredukowanie (dodanie) nowych wartości, \n",
    "2. \"odwrotne zredukowanie\" (odjęcie) wartości które opuściły już okno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46cc2e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/air/Desktop/spark')\n",
    "\n",
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWC4\")\n",
    "ssc = StreamingContext(sc, 2)\n",
    "ssc.checkpoint(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4851e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a69846",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda x: re.findall(r\"[a-z']+\", x.lower()))\n",
    "pairs = words.map(lambda word: (word,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf91985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# window length - długość trwania okna\n",
    "# sliding interval - czas w którym wykonywana jest funkcja okna  \n",
    "\n",
    "windowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)\n",
    "\n",
    "windowedWordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d40ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-05-11 20:18:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 20:18:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 20:18:30\n",
      "-------------------------------------------\n",
      "('is', 5)\n",
      "('engine', 1)\n",
      "('compatible', 1)\n",
      "('hadoop', 3)\n",
      "('run', 1)\n",
      "('in', 2)\n",
      "('clusters', 1)\n",
      "('yarn', 1)\n",
      "(\"spark's\", 1)\n",
      "('mode', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 20:18:40\n",
      "-------------------------------------------\n",
      "('is', 8)\n",
      "('engine', 1)\n",
      "('compatible', 1)\n",
      "('hadoop', 3)\n",
      "('run', 1)\n",
      "('in', 3)\n",
      "('clusters', 1)\n",
      "('yarn', 1)\n",
      "(\"spark's\", 1)\n",
      "('mode', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 20:18:50\n",
      "-------------------------------------------\n",
      "('is', 13)\n",
      "('engine', 2)\n",
      "('compatible', 2)\n",
      "('hadoop', 5)\n",
      "('run', 2)\n",
      "('in', 4)\n",
      "('clusters', 2)\n",
      "('yarn', 2)\n",
      "(\"spark's\", 2)\n",
      "('mode', 2)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 20:19:00\n",
      "-------------------------------------------\n",
      "('is', 14)\n",
      "('than', 10)\n",
      "('complicated', 2)\n",
      "('flat', 2)\n",
      "('nested', 2)\n",
      "('sparse', 1)\n",
      "('readability', 1)\n",
      "('counts', 1)\n",
      "('cases', 1)\n",
      "(\"aren't\", 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 20:19:10\n",
      "-------------------------------------------\n",
      "('although', 4)\n",
      "('way', 3)\n",
      "('may', 3)\n",
      "('at', 2)\n",
      "(\"you're\", 2)\n",
      "('dutch', 2)\n",
      "('now', 3)\n",
      "('is', 13)\n",
      "('than', 9)\n",
      "('never', 4)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 20:19:20\n",
      "-------------------------------------------\n",
      "('process', 2)\n",
      "('in', 4)\n",
      "('hdfs', 2)\n",
      "('cassandra', 2)\n",
      "('hive', 2)\n",
      "('hadoop', 4)\n",
      "('is', 13)\n",
      "('designed', 2)\n",
      "('perform', 2)\n",
      "('both', 2)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 20:19:30\n",
      "-------------------------------------------\n",
      "('sparse', 2)\n",
      "('is', 13)\n",
      "('than', 9)\n",
      "('readability', 2)\n",
      "('counts', 2)\n",
      "('cases', 1)\n",
      "(\"aren't\", 1)\n",
      "('rules', 1)\n",
      "('although', 3)\n",
      "('errors', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 20:19:40\n",
      "-------------------------------------------\n",
      "('although', 4)\n",
      "('never', 4)\n",
      "('is', 14)\n",
      "('than', 9)\n",
      "('right', 2)\n",
      "('now', 3)\n",
      "('idea', 4)\n",
      "('may', 2)\n",
      "('good', 1)\n",
      "('are', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 20:19:50\n",
      "-------------------------------------------\n",
      "('new', 2)\n",
      "('like', 2)\n",
      "('streaming', 2)\n",
      "('machine', 2)\n",
      "('learning', 2)\n",
      "('is', 12)\n",
      "('than', 8)\n",
      "('implicit', 1)\n",
      "('simple', 1)\n",
      "('complicated', 1)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-041d72fe8347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/spark/python/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/Desktop/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "ssc.stop(True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e559d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
