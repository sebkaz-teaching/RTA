{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12259b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# na te zajęcia potrzebować będziemy wersji lokalnej SPARKA (może być docker !)\n",
    "# git clone \n",
    "# docker build -t spark .\n",
    "# docker run -p 8888:8888 spark\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread\n",
    "# and batch interval of 1 second\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\") \\ # TUTAJ WAŻNE ABY BYŁY PRZYNAJMNIEJ DWA PROCESORY\n",
    "        .appName(\"Stream_Socket\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# biblioteka re - regular expression \n",
    "import re\n",
    "# =================\n",
    "# RDD - czyli podstawowy obiekt do obsługi danych dla SPARKA\n",
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Prosty Word Count \n",
    "# r'[a-z'] - oznacza jeden znak dla liter od a do z (male) + apostrof\n",
    "# []+ - oznacza, że może pojawić się jeden lub więcej znaków\n",
    "# pamiętajcie, że przetwarzamy x.lower()\n",
    "# Licznik ten jest wygenerowany na podstawie map-reduce\n",
    "# ===================\n",
    "\n",
    "sc.textFile(\"RDD_input\") \\\n",
    ".map(lambda x: re.findall(r\"[a-z']+\", x.lower())) \\\n",
    ".flatMap(lambda x: [(y, 1) for y in x]) \\\n",
    ".reduceByKey(lambda x,y: x + y) \\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871fec78",
   "metadata": {},
   "source": [
    "## SPARK STREAMING\n",
    "\n",
    "Część Sparka odpowiedzialna za przetwarzanie danych w czasie rzeczywistym. \n",
    "\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-arch.png\"/>\n",
    "\n",
    "Dane mogą pochodzić z różnych źródeł np. sokety TCP, Kafka, etc. \n",
    "Korzystając z poznanych już metod `map, reduce, join, oraz window` można w łatwy sposób generować przetwarzanie strumienia tak jaby był to nieskończony ciąg RDD. \n",
    "Ponadto nie ma problemu aby wywołać na strumieniu operacje ML czy wykresy. \n",
    "\n",
    "Cała procedura przedstawia się następująco: \n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-flow.png\"/>\n",
    "\n",
    "SPARK STREAMING w tej wersji wprowadza abstrakcje zwaną `discretized stream` *DStream* (reprezentuje sekwencję RDD).\n",
    "\n",
    "Operacje na DStream można wykonywać w API JAVA, SCALA, Python, R (nie wszystkie możliwości są dostępne dla Pythona). \n",
    "\n",
    "## Spark Streaming potrzebuje minium 2 rdzenie !\n",
    "\n",
    "----\n",
    "- **StreamingContext(sparkContext, batchDuration)** - reprezentuje połączenie z klastrem i służy do tworzenia DStreamów, `batchDuration` wskazuje na granularność batch'y (w sekundach)\n",
    "- **socketTextStream(hostname, port)** - tworzy DStream na podstawie danych napływających ze wskazanego źródła TCP\n",
    "- **flatMap(f), map(f), reduceByKey(f)** - działają analogicznie jak w przypadku RDD z tym że tworzą nowe DStream'y\n",
    "- **pprint(n)** - printuje pierwsze `n` (domyślnie 10) elementów z każdego RDD wygenerowanego w DStream'ie\n",
    "- **StreamingContext.start()** - rozpoczyna działania na strumieniach\n",
    "- **StreamingContext.awaitTermination(timeout)** - oczekuje na zakończenie działań na strumieniach\n",
    "- **StreamingContext.stop(stopSparkContext, stopGraceFully)** - kończy działania na strumieniach\n",
    "\n",
    "Obiekt StreamingContext można wygenerować za pomocą obiektu SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 5) # ustawiłem na 5 sekund aby był czas na wpisywanie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2697f3f",
   "metadata": {},
   "source": [
    "Po wygenerowaniu obiektu `ssc` musisz wskazać źródło i utworzyć na jego podstawie DStream. Określić wszystkie transformacje. Uruchomić metodę `start()`, która powoduje nasłuchiwanie. Włączyć oczekiwanie na zakończenie procesu `awaitTermination()` bądź zatrzymać nasłuch ręcznie `stop()`. \n",
    "\n",
    "- po rozpoczęciu nasłuchu nie można już ustawić nowych przekształceń !\n",
    "- po zatrzymaniu nie można zrestartować\n",
    "- tylko jeden StreamingContext aktywny na JVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DStream - dane pobierane z socketu TCP - na porcie 9999\n",
    "\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda x: re.findall(r\"[a-z']+\", x.lower()))\n",
    "wordCounts = words.map(lambda word: (word,1)).reduceByKey(lambda x,y: x+y)\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82941575",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate\n",
    "ssc.stop(True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pierwsza wersja nadawania na TCP 9999 (unix)\n",
    "# w konsoli linuxowej netcat Nmap for windows\n",
    "!nc -lk 9999\n",
    "\n",
    "# wpisujesz tekst\n",
    "# jeśli się nie uda to generuj plik poniżej (Możecie też zmienić plik źródłowy)\n",
    "# zweryfikujcie żeby ścieżka była odpowiednia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28565fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file stream.py\n",
    "\n",
    "from socket import *\n",
    "import time\n",
    "\n",
    "rdd = list()\n",
    "with open(\"RDD_input\", 'r') as ad:\n",
    "    for line in ad:\n",
    "        rdd.append(line)\n",
    "\n",
    "HOST = 'localhost'\n",
    "PORT = 9999\n",
    "ADDR = (HOST, PORT)\n",
    "tcpSock = socket(AF_INET, SOCK_STREAM)\n",
    "tcpSock.bind(ADDR)\n",
    "tcpSock.listen(5)\n",
    "\n",
    "\n",
    "while True:\n",
    "    c, addr = tcpSock.accept()\n",
    "    print('got connection')\n",
    "    for line in rdd:\n",
    "        try:\n",
    "            c.send(line.encode())\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            break\n",
    "    c.close()\n",
    "    print('disconnected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w osobnej konsoli (możesz uruchomić ją w jupyter notebook)\n",
    "! python stream.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e879cf8",
   "metadata": {},
   "source": [
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream.png\"/>\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac032a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dane do strumienia można wygenerować też jako listę i wykorzystać tzw kolejkę \n",
    "## kolejka to tak jak w sklepie obsługujemy pierwszego (ostatni dochodzi na koniec kolejki)\n",
    "## po obsłużeniu wypada pierwszy i kolejka się zmniejsza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c35b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\") \\ # TUTAJ WAŻNE ABY BYŁY PRZYNAJMNIEJ DWA PROCESORY\n",
    "        .appName(\"Stream_Kolejka\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 1) # ustawiłem na 5 sekund aby był czas na wpisywanie\n",
    "\n",
    "# 10 pakietów po 1000 liczb\n",
    "rddQueue = []\n",
    "for i in range(10):\n",
    "        rddQueue += [sc.parallelize(\n",
    "            [j for j in range(1, 1001)], 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputStream = ssc.queueStream(rddQueue)\n",
    "\n",
    "mappedStream = inputStream.map(lambda x: (x % 10, 1))\n",
    "reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "reducedStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ad5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tym razem trzeba trochę inaczej uruchomić i wprowadzić czas między\n",
    "import time\n",
    "ssc.start()\n",
    "time.sleep(10)\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d19cc6",
   "metadata": {},
   "source": [
    "## Stateful Wordcount \n",
    "\n",
    "Operacja `updateStateByKey` pozwala łączyć ze sobą wyniki otrzymywane na poszczególbych DStreamach. Dzięki tej operacji możesz w sposób ciągły uzupełniać informacje !\n",
    "\n",
    "Aby Spark Streaming mógł łączyć dane z wielu batchy (stateful transformations) konieczne jest wskazanie lokalizacji gdzie zapisywane będą checkpointy.\n",
    "\n",
    "1. Zdefiniuj stan podstawowy\n",
    "2. wskaż funkcję łączącą \n",
    "\n",
    "----\n",
    "- **checkpoint(directory)** - wskazuje gdzie zapisywane będą checkpointy z operacji na DStream'ach\n",
    "- **updateStateByKey(updateFunc)** - zwraca nowy DStream zawierający informację o bieżącym stanie poszczególnych kluczy, stan każdego klucza odświeżany jest przy pomocy `updateFunc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\") \\ # TUTAJ WAŻNE ABY BYŁY PRZYNAJMNIEJ DWA PROCESORY\n",
    "        .appName(\"Stream_stateful\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutaj zdecydujcie czy puszczacie z pliku czy z konsoli (z konsoli dobrze widac jak dziala)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "words = lines.flatMap(lambda x: re.findall(r\"[a-z']+\", x.lower()))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "runningCounts = pairs.updateStateByKey(updateFunc)\n",
    "\n",
    "runningCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5425302",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "\n",
    "ssc.stop(True,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8999cd8",
   "metadata": {},
   "source": [
    "## Redukcja w oknach \n",
    "\n",
    "----\n",
    "- **reduceByKeyAndWindow(func, invFunc, windowDuration, slideDuration)** - zwraca nowy DStream powstały w wyniku stosowania przyrostowo reduceByKey wewnątrz zdefiniowanego okna. Zredukowane wartości dla nowego okna obliczane są z wykorzystaniem wartości starego okna poprzez: \n",
    "1. zredukowanie (dodanie) nowych wartości, \n",
    "2. \"odwrotne zredukowanie\" (odjęcie) wartości które opuściły już okno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148714ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\") \\ # TUTAJ WAŻNE ABY BYŁY PRZYNAJMNIEJ DWA PROCESORY\n",
    "        .appName(\"Stream_windows\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 2)\n",
    "ssc.checkpoint(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d5099",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "words = lines.flatMap(lambda x: re.findall(r\"[a-z']+\", x.lower()))\n",
    "pairs = words.map(lambda word: (word,1))\n",
    "# window length - długość trwania okna\n",
    "# sliding interval - czas w którym wykonywana jest funkcja okna \n",
    "windowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)\n",
    "\n",
    "windowedWordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e4b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "ssc.stop(True,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29fd5d6",
   "metadata": {},
   "source": [
    "### Stream do DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab1c2c3",
   "metadata": {},
   "source": [
    "lines = DataFrame reprezentujący nieograniczoną tabelę zawierającą dane strumieniowe. \n",
    "Zawiera ona jedną kolumnę o nazwie `value`. Każda nowa linia to wiersz w tabeli. \n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b608a",
   "metadata": {},
   "source": [
    "## SPARK Structured Streaming\n",
    "\n",
    "----\n",
    "- **SparkSession.readStream.format(source).option(key, value).load()** - tworzy strumieniowy DataFrame\n",
    "- **DataFrame.writeStream.outputMode(mode).format(source).option(key, value).start()** - wysyła dane ze strumieniowego DataFrame'u \"na zewnątrz\"; `complete` mode - outputem jest cała zaktualizowana tabela, `append` mode - outputem są jedynie nowe wiersze, `update` mode - outputem są jedynie zaktualizowane wiersze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48925033",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file spark2.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\") \\\n",
    "        .appName(\"Stream_DF\")\\\n",
    "        .getOrCreate()\n",
    "    print(\"=\"*50)\n",
    "    print(\"Zaczynamy DataFrame\")\n",
    "    print(\"=\"*50)\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    lines = spark.readStream\\\n",
    "        .format(\"socket\")\\\n",
    "        .option(\"host\", \"localhost\")\\\n",
    "        .option(\"port\", 9999)\\\n",
    "        .load()\n",
    "    words = lines.select(f.explode(f.split(lines.value, \" \")).alias(\"word\"))\n",
    "    wordCounts = words.groupBy(\"word\").count()\n",
    "    query = wordCounts.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "    query.awaitTermination()\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b20a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "! spark-submit spark2.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
